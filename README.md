# AI Challenge 2025: RL для модели Изинга

Решение задачи **"RL для модели Изинга"** от компании **Росатом** командой **f0rtYtwO** в рамках конкурса **AI Challenge 2025**.

## Описание задачи

Разработка RL-агента для минимизации энергии системы в задаче Изинга путем последовательного изменения зарядов вершин графа.

### Формальная постановка

- Граф на 50 вершинах
- Каждая вершина имеет заряд: $s_i \in \{-1, +1\}$
- Каждое ребро имеет заряд: $J_{ij} \in \{-1, +1\}$
- Суммарная энергия системы:

$$
E(s) = - \sum_{(i,j) \in E} J_{ij} \cdot s_i \cdot s_j
$$

**Цель**: Минимизировать энергию путем последовательного изменения зарядов вершин.

## Решение

### Архитектура

Мы использовали **Deep Q-Network (DQN)** с графовой нейронной сетью на основе **Graph Attention Network (GAT)**.

**Ключевые компоненты:**

1. **Кодирование графа**: Использование матрицы смежности и вектора зарядов вершин
2. **GAT слои**: 3 слоя с attention механизмом для обработки структуры графа
3. **DQN**: Оценка Q-значений для всех возможных действий
4. **Experience Replay**: Для стабилизации обучения
5. **Target Network**: Для уменьшения корреляции в обновлениях

### Основные результаты

- **Максимальный результат на лидерборде** в Round 1
- Эффективная минимизация энергии на тестовых конфигурациях
- Агент обучается находить почти оптимальные решения

## Использование

### Требования

```bash
pip install torch torch-geometric networkx matplotlib numpy tqdm
```

### Запуск

Откройте ноутбук в Google Colab или Jupyter:

```bash
jupyter notebook AIC_2025_Round_1_Solution.ipynb
```

Основные секции ноутбука:

1. **Формализация задачи** - математическая постановка в терминах RL
2. **Environment** - реализация среды для задачи Изинга
3. **Агент (DQN + GAT)** - архитектура нейронной сети
4. **Обучение** - процесс тренировки агента
5. **Валидация** - тестирование на валидационных конфигурациях
6. **Визуализация** - анимация процесса оптимизации

## Визуализация

В ноутбуке представлена интерактивная анимация работы агента, показывающая:
- Изменение энергии системы во времени
- Текущую конфигурацию графа
- Действия агента на каждом шаге

## Методология

### Формализация в терминах RL

- **Состояние**: Конфигурация графа (матрица смежности + вектор зарядов)
- **Действие**: Выбор вершины и инверсия её заряда ($s_i \leftarrow -s_i$)
- **Награда**: Изменение энергии $r = E(s_{\text{до}}) - E(s_{\text{после}})$
- **Политика**: Нейронная сеть $\pi_\theta(a \mid s)$

### Архитектура сети

```
Input (Graph) → GAT Layer 1 → GAT Layer 2 → GAT Layer 3 → FC Layers → Q-values (50)
```

## Команда

**f0rtYtwO**

## Примечания

Несмотря на максимальный результат на лидерборде первого раунда, команда не прошла дальше в конкурсе. Тем не менее, решение демонстрирует эффективное применение методов Reinforcement Learning к задачам комбинаторной оптимизации.

## Лицензия

MIT License

## Благодарности

- Организаторам AI Challenge 2025
- Компании Росатом за интересную задачу
